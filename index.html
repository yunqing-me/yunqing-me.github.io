<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content='width=800'>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22 px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }

    img {
      display: inline;
      margin: 0 auto;
      width: 100%;
    }

    .image-cropper {
      width: 250px;
      height: 250px;
      position: relative;
      overflow: hidden;
      border-radius: 50%;
    }
  </style>
  <link rel="icon" type="image" href="img/artist-palette_1f3a8.png">
  <title>Yunqing Zhao</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <p align="center">
                <name>Yunqing Zhao</name>
                </font>
              <p align>

                I am a research scientist @ TikTok / ByteDance, Singapore.

                I received my PhD (Outstanding Thesis Award) from <a href="https://sutd.edu.sg/">Singapore University of Technology and Design</a> (SUTD) in 2024, 
                advised by Prof. <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man">Ngai-Man Cheung</a>. 
                I work on multimodal / generative AI models, with research interests in (1) synthetic data and (2) cost-efficient large ML models.
                <br>
                <br>
                During my PhD, I had wonderful research experience with 
                <a href="https://duchao0726.github.io/">Chao Du</a> and <a href="https://p2333.github.io/">Tianyu Pang</a> and being advised by <a href="https://yanshuicheng.info/">Prof. Shuicheng Yan</a> and <a href="https://linmin.me/">Min Lin</a>
                at <a href="https://sail.sea.com/research">Sea AI Lab (SAIL)</a>.
                I also had great pleasure to collaborate with <a href="https://henghuiding.github.io/">Henghui Ding</a> and <a href="https://huanghoujing.github.io/">Houjing Huang</a> at Bytedance AI Lab.
                I spent a winter at Microsoft Research - Asia and explored the capabilities/limitations of multimodal foundation models.
                Prior to that, I did my master in <a href="https://www.hku.hk/">University of Hong Kong</a> (Degree with Distinction) and undergraduate in <a href="https://en.cug.edu.cn/">China University of Geosciences</a> (with honors). 
                I visited <a href="https://www.ubc.ca/">UBC @ Vancouver</a> in 2016 summer and I like there.
                <br>
                <br>
                <p style="color: rgba(222, 9, 9, 0.997);">We are looking for strong and self-motivated students / collaborators working on fundamental and interesting AI research problems. 
                  
                  <br> <br> The expected outputs include publications / patents on top-tier academic avenues. Please feel free to drop an email if interested.</p>
              <p align=center>
                <a href="mailto:yunqing.z.0817[AT]gmail[dot]com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/yunqing-zhao/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/yunqing-me">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=kQA0x9UAAAAJ&hl=en">Google Scholar</a>

              </p>
            </td>
            <td width="40%"><img class="image-cropper" src="img/avatar.jpeg"></td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <br>
            <heading style="font-size:22px"> Things happened recently</heading> (occasionally updated)
               <p style="font-size: 12.5px">- <b>[2025-08]:</b> My PhD thesis has been selected as the winner of <a href="https://www.sutd.edu.sg/admissions/graduate/graduate-student-awards/"> Outstanding Thesis Award</a>!</p>
               <p style="font-size: 12.5px">- <b>[2025-07]:</b> Our work on MoE based Video LLMs is accepted by ICCV 2025!</p>
               <p style="font-size: 12.5px">- <b>[2024-07]:</b> I received my PhD Degree from SUTD, and I joined TikTok / ByteDance Singapore as a research scientist.</p>
            <br>
            <br>
          </tr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <br>
            <heading style="font-size:22px"> Recent and Selected Publications</heading>
            <br>
              <p style="font-size:13px"><sup>&dagger;</sup> denotes corresponding author(s); <sup>&#42;</sup> denotes equal contribution; Full list of publications: [<a href="https://scholar.google.com/citations?user=kQA0x9UAAAAJ&hl=en">Google Scholar</a>]</p>
            <br>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/TimeExpert_teaser.png" alt="project_img" width="160" height="100" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>TimeExpert: An Expert-Guided Video LLM for Video Temporal Grounding</papertitle><br>
                <i><span style="font-size: 9pt;">
                    <a href="https://mwxely.github.io/">Zuhao Yang</a>,
                    <a href="https://yingchen001.github.io/">Yingchen Yu</a>,
                    <b>Yunqing Zhao</sup></b>,  
                    <a href="https://personal.ntu.edu.sg/shijian.lu/">Shijian Lu<sup>&dagger;</sup></a>,
                    <a href="https://songbai.site/">Song Bai</a>
                </i><br>
                
                <p>
                  We introduce TimeExpert, a Mixture-of-Experts (MoE)-based Video-LLM that effectively decomposes Video Temporal Grounding (VTG) tasks by dynamically routing task-specific tokens (e.g., timestamps, saliency scores) to specialized experts, with increased computational efficiency. 
                  Our design choices enable precise handling of each subtask, leading to improved event modeling across diverse VTG applications.
                  It achieved <b>State-of-the-art performance on various VTG tasks</b> such as Dense Video Captioning, Moment Retrieval, and Video Highlight Detection.
                </p>

                <b>ICCV 2025, Honolulu, Hawaii, United States.</b><br>
              [<a href="https://arxiv.org/abs/2508.01699">Paper</a>]
              [<a href="https://mwxely.github.io/projects/yang2025time/index">Webpage</a>]
              [<a href="https://mwxely.github.io/projects/yang2025time/index">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/method-watermark.jpg" alt="project_img" width="160" height="100" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>A Recipe for Watermarking (Multimodal) Diffusion Models</papertitle><br>
                <i><span style="font-size: 9pt;">
                    <b>Yunqing Zhao</sup></b>, 
                    <a href="https://p2333.github.io/">Tianyu Pang</sup><sup>&dagger;</sup></a>, 
                    <a href="https://duchao0726.github.io/">Chao Du<sup>&dagger;</sup></a>, 
                    <a href="https://ml.cs.tsinghua.edu.cn/~xiaoyang/">Xiao Yang</a>, 
                    <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>,
                    <a href="https://linmin.me/">Min Lin</a>
                </i><br>
                
                <p>
                  We conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-theart DMs (e.g., Stable Diffusion), via training from scratch or finetuning. 
                  Our recipe is straightforward but involves empirically ablated implementation details, providing a solid foundation for future research on watermarking DMs.
                </p>

                <b>arXiv, 2023 & US Patent, 2024</b><br>
              [<a href="https://arxiv.org/abs/2303.10137">Paper</a>]
              [<a href="https://yunqing-me.github.io/WatermarkDM/">Webpage</a>]
              [<a href="https://github.com/yunqing-me/WatermarkDM">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/AttackVLM.jpeg" alt="project_img" width="160" height="100" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>Evaluating adversarial robustness of large vision-language models (VLMs)</papertitle><br>
                <i><span style="font-size: 9pt;">
                    <b>Yunqing Zhao<sup>&#42;</sup></b>, 
                    <a href="https://p2333.github.io/">Tianyu Pang<sup>&#42;</sup><sup>&dagger;</sup></a>, 
                    <a href="https://duchao0726.github.io/">Chao Du<sup>&dagger;</sup></a>, 
                    <a href="https://ml.cs.tsinghua.edu.cn/~xiaoyang/">Xiao Yang</a>, 
                    <a href="https://zhenxuan00.github.io/">Chongxuan Li</a>, 
                    <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>,
                    <a href="https://linmin.me/">Min Lin</a>
                </i><br>
                
                <p>
                  Large VLMs such as <b>GPT-4</b> achieve unprecedented performance in response generation, 
                  esp. with visual inputs, enabling more creative and adaptable interaction than <b>LLMs</b> like <b>ChatGPT</b>. 
                  However, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision).
                  We evaluate the robustness of open-source large VLMs (<b>e.g., MiniGPT-4, LLaVA, BLIP, UniDiffuser</b>) in the most realistic and high-risk setting, 
                  where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses.
                </p>

                <b>NeurIPS 2023, New Orleans, Louisiana, United States.</b><br>
              [<a href="https://arxiv.org/pdf/2305.16934.pdf">Paper</a>]
              [<a href="https://yunqing-me.github.io/AttackVLM/">Webpage</a>]
              [<a href="https://github.com/yunqing-me/AttackVLM">Code</a>]
              </a> </p>
            </td>
          </tr>
          <tr>
            <td width="30%">
              <img id="img-opt" src="img/incompatible-transfer.png" alt="project_img" width="160" height="100" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>Exploring incompatible knowledge transfer in few-shot image generation</papertitle><br>
                <i><span style="font-size: 9pt;">
                    <b>Yunqing Zhao</b>, 
                    <a href="https://duchao0726.github.io/">Chao Du</a>, 
                    <a href="https://miladabd.github.io/">Milad Abdollahzadeh</a>, 
                    <a href="https://p2333.github.io/">Tianyu Pang</a>, 
                    <a href="https://linmin.me/">Min Lin</a>,
                    <a href="https://yanshuicheng.info/">Shuicheng Yan</a>,
                    <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>
                </i><br>

              <p>Through interpretable GAN dissection tools, we demonstrate that fine-tuning based methods cannot effectively remove knowledge that is incompatible
                to the target domain after adaptation (e.g., trees /buildings on the sea) for few-shot image generation task.
                We propose Remove In-Compatible Knowledge (RICK), an efficient and dynamic algorithm that estimates the filter importance and prune those are incompatible 
                to the target domain.
                <br>

              </p>
              <b>CVPR 2023, Vancouver, British Columbia, Canada.</b><br>
              [<a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Zhao_Exploring_Incompatible_Knowledge_Transfer_in_Few-Shot_Image_Generation_CVPR_2023_paper.pdf">Paper</a>]
              [<a href="https://yunqing-me.github.io/RICK/">Webpage</a>]
              [<a href="https://github.com/yunqing-me/RICK">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/fs-ban-method.png" alt="project_img" width="160" height="100" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>FS-BAN: Born-Again Networks for Domain Generalization Few-shot Classification</papertitle><br>
                <i><span style="font-size: 9pt;">
                  <b>Yunqing Zhao</b>, 
                  <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>
              </i><br>

              <p>We propose a method to improve the generalizability for cross-domain few-shot classification problem using born-again networks.
                Our algorithm does not require additional parameters and training data and can be applied readily to many exisiting FSC models.
                The key insight is to distill the dark knowledge from a teacher model with additional multi-task objectives designed specific for 
                cross-domain few-shot learning.
              <br>
              </p>

            <b>IEEE Trans. on Image Processing (TIP) 2023.</b> <br>
              [<a href="https://arxiv.org/abs/2208.10930">Paper</a>]
              [<a href="https://yunqing-me.github.io/Born-Again-FS/">Webpage</a>]
              [<a href="https://github.com/yunqing-me/Born-Again-FS">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/adam-compare.png" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>Few-shot image generation via adaptation-aware kernel modulation</papertitle><br>
                <i><span style="font-size: 9pt;">
                  <b>Yunqing Zhao</b><sup>&#42;</sup>, 
                  <a href="https://keshik6.github.io/">Keshigeyan Chandrasegaran<sup>&#42;</sup></a>, 
                  <a href="https://miladabd.github.io/">Milad Abdollahzadeh<sup>&#42;</sup></a>, 
                  <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>
              </i><br>

              <p>When fine-tuning a pretrained image generator on few-shot target samples, we show that state-of-the-art algorithms perform no-better
                than a simple baseline method when the target samples are distant to the source domain. 
                We propose AdAM, a parameter-efficient and target-aware method to select source knowledge important for few-shot adaptation.
              <br>
              </p>

              <b>NeurIPS 2022, New Orleans, Louisiana, United States.</b><br>
              [<a href="https://arxiv.org/abs/2210.16559">Paper</a>]
              [<a href="https://yunqing-me.github.io/AdAM/">Webpage</a>]
              [<a href="https://github.com/yunqing-me/AdAM">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/ls-kd-compatibility.png" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>Revisiting Label Smoothing & Knowledge Distillation Compatibility: What was Missing?</papertitle><br>
                <i><span style="font-size: 9pt;">
                  <a href="https://keshik6.github.io/">Keshigeyan Chandrasegaran</a>, 
                  <a href="https://scholar.google.com.sg/citations?user=9SE3GYMAAAAJ&hl=en">Ngoc-Trung Tran<sup>&#42;</sup></a>, 
                  <b>Yunqing Zhao<sup>&#42;</sup></b>, 
                  <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>
              </i><br>

              <p>We investigate the compatibility between label smoothing (LS) and knowledge distillation (KD), i.e., to smooth or not to smooth a teacher network?
                We discover, analyze and validate the proposed systematic diffusion as the missing concept which is instrumental in understanding and resolving these contradictory findings in prior works. 
                This systematic diffusion essentially curtails the benefits of distilling from an LS-trained teacher, thereby rendering KD at increased temperatures ineffective.

              <br>
              </p>

              <b>ICML 2022, Baltimore, Maryland, United States.</b><br>
              [<a href="https://proceedings.mlr.press/v162/chandrasegaran22a/chandrasegaran22a.pdf">Paper</a>]
              [<a href="https://keshik6.github.io/revisiting-ls-kd-compatibility/">Webpage</a>]
              [<a href="https://github.com/sutd-visual-computing-group/LS-KD-compatibility">Code</a>]
              </a> </p>
            </td>
          </tr>

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/a-closer-look-fsig.png" alt="project_img" width="160" height="100" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><papertitle>A Closer Look at Few-shot Image Generation</papertitle><br>
                <i><span style="font-size: 9pt;">
                  <b>Yunqing Zhao</b>, 
                  <a href="https://henghuiding.github.io/">Henghui Ding</a>, 
                  <a href="https://huanghoujing.github.io/">Houjing Huang</a>, 
                  <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung<sup>&dagger;</sup></a>
              </i><br>

              <p>We analyze the existing few-shot image generation algorithms in a unified testbed and find that
                diversity degradation is the major issue during few-shot target adaptation. 
                Our proposed mutual information based algorithm can alleviate this issue and achieve state-of-the-art performance
                on few-shot image generation tasks.
              <br>
              </p>

              <b>CVPR 2022, New Orleans, Louisiana, United States.</b><br>
              [<a href="https://arxiv.org/abs/2205.03805">Paper</a>]
              [<a href="https://yunqing-me.github.io/A-Closer-Look-at-FSIG/">Webpage</a>]
              [<a href="https://github.com/yunqing-me/A-Closer-Look-at-FSIG">Code</a>]
              </a> </p>
            </td>
          </tr>


            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <br>
                <br>
                <br>
                <heading style="font-size:22px"> Workshop & Challenge </heading>
                <br>
                <br>
                <br>
              </tr>
    
              <tr>
                <td width="30%">
                  <img id="img-opt" src="img/exp-fsc-20.png" alt="project_img" width="160" height="100" style="border-style: none">
                </td>
    
                <td valign="top" width="70%">
                  <p><papertitle>Explanation-guided Training for Cross-domain Few-shot Classification</papertitle><br>
                    <i><span style="font-size: 9pt;">
                      Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, <b>Yunqing Zhao</b>, 
                      <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/">Ngai-Man Cheung</a>,
                      <a href="https://scholar.google.de/citations?user=5B8CTlEAAAAJ&hl=de">Alexander Binder<sup>&dagger;</sup></a>
                    </i><br>
                    <br><br>
                    ICML-2020 Workshop & ICPR-2020.
                    <br>
                  [<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412941">Paper</a>]
                  [<a href="https://github.com/SunJiamei/few-shot-lrp-guided">Code</a>]
                  </a> </p>
                </td>
              </tr>

              <tr>
                <td width="30%">
                  <img id="img-opt" src="img/cikm20.png" alt="project_img" width="160" height="100" style="border-style: none">
                </td>
    
                <td valign="top" width="70%">
                  <p><papertitle>CIKM-2020 Alibaba-Tsinghua Adversarial Challenge on Object Detection</papertitle><br>
                    <i><span style="font-size: 9pt;">
                      <a href="https://scholar.google.com/citations?user=VJYS9dMAAAAJ&hl=zh-CN">Honglin Li</a>, 
                      <b>Yunqing Zhao</b>
                    </i><br><br>
                    Rank 10/1814 in the Challenge
                    <br><br>
                    CIKM-2020 Workshop.
                    <br>
                  [<a href="https://ceur-ws.org/Vol-2881/paper5.pdf">Paper</a>]
                  [<a href="https://github.com/invoker-LL/L0-attack-on-object-detector-NMS">Code</a>]
                  </a> </p>
                </td>
              </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <br>
              <br>
              <br>
              <heading style="font-size:22px"> Selected Research Experience </heading>
              <br>
              <br>
              <br>
            </tr>
            <tr>
              <td>
                <img id="img-opt" src="img/Microsoft-Research.jpg" alt="project_img" width="160" style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <p><papertitle> Microsoft Research - Asia
                  <br>
                  Research Intern</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    11.2023 - 02.2024, 
                  </i>
                  <br>
  
                Exploring the applications/capability of LLMs for multi-modal understanding and generation.
                </a> 
                </p>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img id="img-opt" src="img/logo-sea-header-desktop.png" alt="project_img" width="160" style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <p><papertitle>
                  Sea AI Lab, Singapore
                  <br>
                  Research Intern</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    09.2022 - 11.2023, 
                  </i><br>
  
                Work with <a href="https://duchao0726.github.io/">Chao Du</a> and <a href="https://p2333.github.io/">Tianyu Pang</a><br>
                Advised by <a href="https://yanshuicheng.info/">Prof. Shuicheng Yan</a> and <a href="https://linmin.me/">Min Lin</a><br>
                </a> </p>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img id="img-opt" src="img/tiktok-logo-4500.svg" alt="project_img" width="20" height="60"  style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <p><papertitle>
                  TikTok / ByteDance AI Lab, Singapore
                  <br>
                  Research Intern</papertitle><br><br>
                  <i><span style="font-size: 9pt;">
                    08.2021 - 08.2022, 
                  </i><br>
  
                Work with <a href="https://henghuiding.github.io/">Henghui Ding</a> (now @ Fudan University, Shanghai) and <a href="https://huanghoujing.github.io/">Houjing Huang</a> (now @ UZH, Switzerland) <br>
                </a> </p>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img id="img-opt" src="img/st-engineering.png" alt="project_img" width="20" height="30"  style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <p>
                  <papertitle>ST Engineering - SUTD Cyber Security Lab, Singapore
                  <br> 
                  Student Researcher
                  <br>
                  </papertitle>
                  <br>
                  <i><span style="font-size: 9pt;">
                    08.2020 - 07.2021, 
                  </i>
                  <br>
                Advised by <a href="https://www.sutd.edu.sg/profile/cheung-ngai-man/"> Prof. Ngai-Man Cheung</a>
                <br>
                </p>
              </td>
            </tr>

            <tr>
              <td width="30%">
                <img id="img-opt" src="img/hku-logo.png" alt="project_img" width="20"  style="border-style: none">
              </td>
  
              <td valign="top" width="70%">
                <p><papertitle>University of Hong Kong
                  <br>
                  Research Assistant </papertitle><br><br>
                  
                  Spent wonderful days in SouthLane & Pok Fu Lam Road
                  <br>
                  <i><span style="font-size: 9pt;">
                    11.2018 - 04.2019, 
                  </i><br>

  
                Advised by <a href="https://www.eee.hku.hk/~vtam/">Dr. Vincent Tam</a><br>
                </a> </p>
              </td>
            </tr>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <br>
                <br>
                <br>
                <heading style="font-size:22px"> Education </heading>
                <br>
                <br>
              </tr>
    
              <tr style="position: relative;">
                <td width="30%">
                  <img id="img-opt" style="position: relative;top:-30px;" src="img/sutd-logo.svg" alt="project_img" style="border-style: none">
                </td>
    
                <td valign="top" width="70%">
                  <p><papertitle>Singapore University of Technology and Design</papertitle><br>
                    8 Somapah Road, Singapore 487372<br><br>
                    <i><span style="font-size: 10pt;">
                      Ph.D in AI and Machine Learning.
                    </i><br>
                    Jan. 2020 - July 2024
                  </a> </p>
                  <b>About SUTD:</b>
                  <br>
                  Founded with <a href="https://www.mit.edu/">MIT</a> and started in 2012, SUTD is ranked
                  <br> <b>#66</b> globally in AI, Computer Vision and NLP on <a href="https://csrankings.org/#/fromyear/2018/toyear/2023/index?ai&vision&nlp&world">CSRanking</a> in 2020-2024.
                  <br>
                  <br>
                  <br>
                  <br>
                </td>
                
              </tr>

              <tr style="position: relative;top:-40px;">
                <td width="30%">
                  <img id="img-opt" style="position: relative;top:-0px;" src="img/hku-logo.png" alt="project_img" style="border-style: none">
                </td>
                <td valign="top" width="70%">
                  <p><papertitle>The University of Hong Kong</papertitle><br>
                    Pok Fu Lam Road, Hong Kong<br><br>
                    <i><span style="font-size: 10pt;">
                      M.S in Electrical and Electronic Engineering (<b>degree with Distinction</b>)
                      <br>
                    </i>
                  I have a wonderful visiting experience in Zhejiang University, Hangzhou, China.
                  </a> </p>
                </td>
                <br>
              </tr>

              <tr style="position: relative;top:-40px;">
                <td width="30%">
                  <img id="img-opt" style="position: relative;top:0px;" src="img/cug-logo.png" alt="project_img" style="border-style: none">
                </td>
    
                <td valign="top" width="70%">
                  <p><papertitle>China University of Geosciences</papertitle><br>
                    388 Lumo Road, Wuhan, 430074<br><br>
                    <i><span style="font-size: 10pt;">
                      B.Eng in Automation<br>
                    </i>
                    I spent a wonderful summer semester in University of British Columbia, Vancouver BC, Canada.
                  </a> </p>
                </td>
              </tr>

              <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                  <br>
                  <heading style="font-size:22px"> Teaching & Service </heading>
                  <br>
                </tr>
      
                <tr>
                  <td valign="top" width="70%">
                    <p><papertitle>Active Reviewer</papertitle> of 
                      NeurIPS, CVPR, TPAMI, TIP, TIFS, TNNLS, TASL, TMM, TCSVT, CVIU, etc.
                    </p>
                    <p><papertitle>Graduate Teaching Assistant</papertitle> of 
                      <a href="https://www.sutd.edu.sg/course/50-021-artificial-intelligence"> 50.021 Artificial Intelligence</a> 
                      and
                      <a href="https://www.sutd.edu.sg/course/50-035-computer-vision/"> 50.035 Computer Vision</a> @ SUTD
                    </p>
                  </td>
                </tr>
                
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="2">
                    <a href="http://www.cs.berkeley.edu/~barron/">web template taken from this website</a>
                  </font>
                </p>
              </td>
            </tr>
          </table>

      </td>
    </tr>
  </table>

  <script type="text/javascript">
    var sc_project = 11673319;
    var sc_invisible = 1;
    var sc_security = "327094c7"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="http://statcounter.com/" target="_blank"><img class="statcounter"
          src="//c.statcounter.com/11673319/0/327094c7/1/" alt="Web
Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->
</body>

</html>