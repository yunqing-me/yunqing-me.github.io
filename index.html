<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content='width=800'>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
      color: #1772d0;
      text-decoration: none;
    }

    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }

    body,
    td,
    th {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px
    }

    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 12px;
    }

    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22 px;
    }

    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 13px;
      font-weight: 700
    }

    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }

    .fade {
      transition: opacity .2s ease-in-out;
      -moz-transition: opacity .2s ease-in-out;
      -webkit-transition: opacity .2s ease-in-out;
    }



    img {
      display: inline;
      margin: 0 auto;
      width: 100%;
    }

    .image-cropper {
      width: 250px;
      height: 250px;
      position: relative;
      overflow: hidden;
      border-radius: 50%;
    }
  </style>
  <link rel="icon" type="image" href="img/logo.jpg">
  <title>Nupur Kumari</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Nupur Kumari</name>
                </font>
              <p align>I am a first year PhD student at Robotics Institute, Carnegie Mellon University (CMU). I am
                advised by <a href="https://www.cs.cmu.edu/~junyanz/"> Jun-Yan Zhu</a> and collaborate closely with
                <a href="https://richzhang.github.io/">Richard Zhang</a> and <a
                  href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>. My research interests lie
                in computer vision specifically generative models, self-supervision, and few-shot learning.
                <br>
                <br>
                Prior to CMU, I worked at Media and Data Science Research, Adobe India, and had the pleasure to
                collaborate with <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a> during that
                time.
                I did my undergraduate from Indian Institute of Tenchnology Delhi with a major in Mathematics and
                Computing.

              <p align=center>
                <a href="mailto:nupurkmr9@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/nupur-kumari-582369112/">LinkedIn</a> &nbsp/&nbsp
                <a href="files/resume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=vRWKLJ8AAAAJ&hl=en">Google Scholar</a>

              </p>
            </td>
            <td width="40%"><img class="image-cropper" src="img/photo.jpg"></td>
          </tr>
        </table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <br>
            <br>
            <br>
            <heading style="font-size:22px"> Selected Publications</heading>
            <br>
            <br>
          </tr>


          <tr>
            <td width="30%">
              <img id="img-opt" src="img/custom_diffusion.gif" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="http://arxiv.org/abs/2212.04488">
                  <papertitle>Multi-Concept Customization of Text-to-Image Diffusion</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://zhangbingliang2019.github.io">Bingliang Zhang</a>, <a
                      href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose Custom Diffusion, a method to fine-tune large-scale text-to-image diffusion models e.g. <a
                  href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">Stable Diffusion</a> given few
                (~4-20) user-provided images of a new concept.
                Our method is computationally efficient (~6 minutes on 2 A100 GPUs) and has low storage requirements for
                each additional concept model (75MB) apart from the pretrained model.
                <br>

              </p>
              CVPR 2023.<br>
              [<a href="http://arxiv.org/abs/2212.04488">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~custom-diffusion/">Webpage</a>]
              [<a href="https://github.com/adobe-research/custom-diffusion">Code</a>]
              </a> </p>
            </td>
          </tr>


          <tr>
            <td width="30%">
              <img id="img-opt" src="img/concept_ablation.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2303.13516">
                  <papertitle>Ablating Concepts in Text-to-Image Diffusion Models</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://zhangbingliang2019.github.io">Bingliang Zhang</a>, <a
                      href="https://peterwang512.github.io">Sheng-Yu Wang</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We propose a method to ablate (remove) copyrighted materials and memorized images from pretrained
                text-to-image generative models. Our algorithm changes the target concept distribution to an anchor
                concept, e.g., Van Gogh painting to paintings or Grumpy cat to Cat.
                <br>

              </p>
              Arxiv 2023.<br>
              [<a href="https://arxiv.org/abs/2303.13516">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~concept-ablation/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/concept-ablation">Code</a>]
              </a> </p>
            </td>
          </tr>



          <!-- <tr>
            <td width="30%">
              <img id="img-opt" src="img/modelverse2022.jpg" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://modelverse.cs.cmu.edu/">
                  <papertitle>Content-Based Search for Deep Generative Models</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    <a href="https://github.com/daohanlu">Daohan Lu*</a>, <a
                      href="https://peterwang512.github.io/">Sheng-Yu Wang*</a>,
                    Nupur Kumari*, <a href="https://www.ri.cmu.edu/ri-people/rohan-agarwal/">Rohan Agarwal*</a>, <a
                      href="https://people.csail.mit.edu/davidbau/home/">David Bau</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p> We propose an algorithm for searching over generative models using image,text, and sketch.
                Our search platform is available at <a href="https://modelverse.cs.cmu.edu/">Modelverse</a>.
              </p>
              Arxiv 2022.<br>
              [<a href="https://arxiv.org/abs/2210.03116">Paper</a>]
              [<a href="https://modelverse.cs.cmu.edu/about">Webpage</a>]
              [<a href="https://github.com/generative-intelligence-lab/modelverse">Code</a>]
              </a> </p>
            </td>
          </tr>


 -->

          <tr>
            <td width="30%">
              <img id="img-opt" src="img/vision_aided_gan.png" alt="project_img" width="160" style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/2112.09130">
                  <papertitle>Ensembling Off-the-shelf Models for GAN Training</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari, <a href="https://richzhang.github.io/">Richard Zhang</a>, <a
                      href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a
                      href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a></i><br>

              <p>We show that pretrained computer vision models can significantly improve performance when used in an
                ensemble of discriminators.
                Our method improves FID by 1.5x to 2x on cat, church, and horse categories of LSUN.
                <br>

              </p>
              In CVPR 2022 (Oral).<br>
              [<a href="https://arxiv.org/abs/2112.09130">Paper</a>]
              [<a href="https://www.cs.cmu.edu/~vision-aided-gan/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/vision-aided-gan">Code</a>]

              </a> </p>
            </td>
          </tr>


          <tr>
            <td width="30%"><img id="img-opt" src="img/robust_attr.png" alt="project_img" width="160"
                style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/abs/1911.13073">
                  <papertitle>Attributional Robustness Training using Input-Gradient Spatial Alignment</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Nupur Kumari<sup>&#42;</sup>, <a href="https://msingh27.github.io/">Mayank
                      Singh</a><sup>&#42;</sup>, Puneet Mangla, <a href="https://a7b23.github.io/">Abhishek Sinha</a>,
                    <a href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a>, Balaji
                    Krishnamurthy</i><br>

              <p>We propose a robust attribution training methodology <i>ART</i> that maximizes the alignment between
                the input and its attribution map.
                <i>ART</i> achieves state-of-the-art performance in attributional robustness and weakly supervised
                object localization on CUB dataset.
                <br>
              </p>
              In ECCV 2020. <br>
              [<a href="https://arxiv.org/abs/1911.13073">Paper</a>]
              [<a href="https://nupurkmr9.github.io/Attributional-Robustness/">Webpage</a>]
              [<a href="https://github.com/nupurkmr9/Attributional-Robustness">Code</a>]

              </a> </p>
            </td>
          </tr>



          <tr>
            <td width="30%"><img id="img-opt" src="img/few_shot.png" alt="project_img" width="160"
                style="border-style: none">
            </td>

            <td valign="top" width="70%">
              <p><a href="https://arxiv.org/pdf/1907.12087.pdf">
                  <papertitle>Charting the Right Manifold: Manifold Mixup for Few-shot Learning</papertitle>
                </a><br>
                <i><span style="font-size: 10pt;">
                    Puneet Mangla<sup>&#42;</sup>, Nupur Kumari<sup>&#42;</sup>, <a
                      href="https://a7b23.github.io/">Abhishek Sinha</a><sup>&#42;</sup>, <a
                      href="https://msingh27.github.io/">Mayank Singh</a><sup>&#42;</sup>, <a
                      href="https://www.iith.ac.in/~vineethnb/">Vineeth N Balasubramanian</a>, Balaji
                    Krishnamurthy</i><br>
              <p>Used self-supervision techniques - rotation and exemplar, followed by manifold mixup for few-shot
                classification tasks.
                The proposed approach beats the current state-of-the-art accuracy on mini-ImageNet, CUB and CIFAR-FS
                datasets by 3-8%.
                <br>
              </p>
              In WACV, 2020. <br>
              [<a href="https://arxiv.org/pdf/1907.12087.pdf">Paper</a>]
              [<a href="https://github.com/nupurkmr9/S2M2_fewshot">Code</a>]
              </a> </p>
            </td>
          </tr>


          <table width="100%" align="left" border="0" cellspacing="0" cellpadding="20">
            <p align="left">
              <font size="2">
                <sup>&#42;</sup> denotes equal contribution
              </font>

          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="2">
                    <a href="http://www.cs.berkeley.edu/~barron/">web template taken from this website</a>
                  </font>
                </p>
              </td>
            </tr>
          </table>
      </td>
    </tr>
  </table>

  <script type="text/javascript">
    var sc_project = 11673319;
    var sc_invisible = 1;
    var sc_security = "327094c7"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics
Made Easy - StatCounter" href="http://statcounter.com/" target="_blank"><img class="statcounter"
          src="//c.statcounter.com/11673319/0/327094c7/1/" alt="Web
Analytics Made Easy - StatCounter"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->
</body>

</html>